import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import KFold, train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder  # Import OneHotEncoder
from sklearn.impute import SimpleImputer

# Load and preprocess your dataset (replace with your dataset)
data = pd.read_csv('/Users/udayrowthu/Downloads/test.csv')

# Assuming the last column is the target label and the rest are features
X = data.iloc[:, :-1]  # Features
y = data.iloc[:, -1]    # Target labels

# Separate numeric and categorical columns (if needed)
numeric_columns = X.select_dtypes(include=np.number)
categorical_columns = X.select_dtypes(exclude=np.number)

# Handle missing values for numeric columns using the mean strategy
numeric_imputer = SimpleImputer(strategy='mean')
X[numeric_columns.columns] = numeric_imputer.fit_transform(numeric_columns)

# Encode categorical variables using OneHotEncoder (if needed)
encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
categorical_encoded = encoder.fit_transform(X[categorical_columns.columns])
categorical_encoded_df = pd.DataFrame(categorical_encoded, columns=encoder.get_feature_names(categorical_columns.columns))

# Concatenate the one-hot encoded DataFrame with the numeric data
X = pd.concat([X[numeric_columns.columns], categorical_encoded_df], axis=1)

# Initialize the StandardScaler and fit/transform the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Determine the best K value
k_values = range(1, 21)  # You can adjust the range as needed
accuracies = []

for k in k_values:
    knn_classifier = KNeighborsClassifier(n_neighbors=k)
    knn_classifier.fit(X_train, y_train)
    y_pred = knn_classifier.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)

best_k = k_values[np.argmax(accuracies)]
best_accuracy = max(accuracies)

# Create a visualization of accuracy vs. K value
plt.plot(k_values, accuracies)
plt.xlabel('K Value')
plt.ylabel('Accuracy')
plt.title('Accuracy vs. K Value')
plt.show()

# Run 5-fold cross-validation
cv = KFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = []

for k in k_values:
    knn_classifier = KNeighborsClassifier(n_neighbors=k)
    scores = []
    for train_index, test_index in cv.split(X_scaled):
        X_train_cv, X_test_cv = X_scaled[train_index], X_scaled[test_index]
        y_train_cv, y_test_cv = y[train_index], y[test_index]
        knn_classifier.fit(X_train_cv, y_train_cv)
        y_pred_cv = knn_classifier.predict(X_test_cv)
        accuracy_cv = accuracy_score(y_test_cv, y_pred_cv)
        scores.append(accuracy_cv)
    cv_scores.append(scores)

# Report mean and standard deviation of cross-validation scores
cv_mean = np.mean(cv_scores, axis=1)
cv_std = np.std(cv_scores, axis=1)

# Train the KNN model with the best K value on the entire dataset
final_knn_classifier = KNeighborsClassifier(n_neighbors=best_k)
final_knn_classifier.fit(X_scaled, y)

# Evaluate the model using a confusion matrix
y_pred_final = final_knn_classifier.predict(X_test)
conf_matrix = confusion_matrix(y_test, y_pred_final)
print("Confusion Matrix:")
print(conf_matrix)


#Markdown cell
## Model Accuracy Explanation

The K-nearest neighbors (KNN) classifier was trained and evaluated on the provided dataset. The model's accuracy was assessed using various techniques, including cross-validation and a confusion matrix.

### 1. Best K Value Determination

The accuracy of the model was evaluated for different values of K (number of neighbors). A plot was created to visualize the relationship between the K values and the corresponding accuracies. The best K value was determined as [best_k], where the accuracy was the highest.

### 2. Cross-Validation

To ensure the robustness of the model, 5-fold cross-validation was performed. The mean and standard deviation of the cross-validation scores were calculated. This process helps in assessing how well the model generalizes to new, unseen data.

### 3. Confusion Matrix

The final KNN model, trained with the best K value, was evaluated using a confusion matrix. The confusion matrix provides insights into the model's performance, indicating the number of true positives, true negatives, false positives, and false negatives.

### Conclusion

The overall accuracy of the KNN model, considering the best K value and cross-validation results, suggests that the model performs [provide insights on the model's accuracy and reliability].



K=1: Mean Accuracy = 0.38, Std Deviation = 0.03
K=2: Mean Accuracy = 0.28, Std Deviation = 0.03
K=3: Mean Accuracy = 0.28, Std Deviation = 0.03
K=4: Mean Accuracy = 0.26, Std Deviation = 0.03
K=5: Mean Accuracy = 0.34, Std Deviation = 0.16
K=6: Mean Accuracy = 0.33, Std Deviation = 0.16
K=7: Mean Accuracy = 0.50, Std Deviation = 0.21
K=8: Mean Accuracy = 0.50, Std Deviation = 0.22
K=9: Mean Accuracy = 0.67, Std Deviation = 0.04
K=10: Mean Accuracy = 0.67, Std Deviation = 0.04
K=11: Mean Accuracy = 0.65, Std Deviation = 0.03
K=12: Mean Accuracy = 0.65, Std Deviation = 0.03
K=13: Mean Accuracy = 0.65, Std Deviation = 0.03
K=14: Mean Accuracy = 0.65, Std Deviation = 0.03
K=15: Mean Accuracy = 0.65, Std Deviation = 0.03
K=16: Mean Accuracy = 0.65, Std Deviation = 0.03
K=17: Mean Accuracy = 0.65, Std Deviation = 0.03
K=18: Mean Accuracy = 0.65, Std Deviation = 0.03
K=19: Mean Accuracy = 0.65, Std Deviation = 0.03
K=20: Mean Accuracy = 0.65, Std Deviation = 0.03




Confusion Matrix:
[[20  0  0]
 [ 7  0  1]
 [ 0  0 56]]


